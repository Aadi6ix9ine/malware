import argparse
import pickle
import warnings
global prediction

warnings.filterwarnings("ignore")
def sanitization(web):
    web = web.lower()
    token = []
    dot_token_slash = []
    raw_slash = str(web).split('/')
    for i in raw_slash:
        raw1 = str(i).split('-')
        slash_token = []
        for j in range(0, len(raw1)):
            raw2 = str(raw1[j]).split('.')
            slash_token = slash_token + raw2
        dot_token_slash = dot_token_slash + raw1 + slash_token
    token = list(set(dot_token_slash))
    if 'com' in token:
        token.remove('com')
    return token

def predict_url(url):
    # Using whitelist filter as the model fails in many legit cases since the biggest problem is not finding the malicious urls but to segregate the good ones
    whitelist = ['hackthebox.eu', 'root-me.org', 'gmail.com']
    s_url = [url]  # Use the provided URL

    # Loading the model
    with open("Classifier/pickel_model.pkl", 'rb') as f1:
        lgr = pickle.load(f1)

    with open("Classifier/pickel_vector.pkl", 'rb') as f2:
        vectorizer = pickle.load(f2)

    # Predicting
    x = vectorizer.transform(s_url)
    y_predict = lgr.predict(x)

    for site in whitelist:
        s_url.append(site)

    predict = list(y_predict)
    for j in range(0, len(whitelist)):
        predict.append('good')

    return predict[0]

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='URL Malware Detection')
    parser.add_argument('--url', required=True, help='URL to check for malware')

    args = parser.parse_args()
    url_to_check = args.url

    prediction = predict_url(url_to_check)
    print(prediction)
